name: Trigger Scrape Function Chain

on:
  push:
    branches: [ main ]
  schedule:
    - cron: '*/30 * * * *'

jobs:
  scrape_pages:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        page: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
      max-parallel: 1  # Adjust this based on your needs and rate limits
    steps:
    - name: Trigger Scrape for Page ${{ matrix.page }}
      run: |
        max_retries=3
        retry_count=0
        while [ $retry_count -lt $max_retries ]; do
          response=$(curl -s -w "\n%{http_code}" "https://used-car-scraper-vercel.vercel.app/api/scrape?page=${{ matrix.page }}")
          status_code=$(echo "$response" | tail -n1)
          body=$(echo "$response" | sed '$d')
          echo "Status code: $status_code"
          echo -e "Response body:\n$body"
          if [ $status_code -eq 200 ]; then
            echo "Scrape function for page ${{ matrix.page }} initiated successfully"
            break
          else
            echo "Scrape function for page ${{ matrix.page }} failed with status code: $status_code"
            echo -e "Error details:\n$body"
            retry_count=$((retry_count+1))
            if [ $retry_count -lt $max_retries ]; then
              echo "Retrying in 5 seconds..."
              sleep 5
            else
              echo "Max retries reached. Exiting with failure."
              exit 1
            fi
          fi
        done

    # Optional: Check status of background processing
    # This step would require implementing a status check endpoint in your API
    # - name: Check Processing Status
    #   run: |
    #     echo "Checking processing status for page ${{ matrix.page }}"
    #     # Implement status check logic here
    #     # This could involve polling a status endpoint until processing is complete
    #     # or for a maximum number of attempts